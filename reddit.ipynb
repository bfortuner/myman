{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from punisher.common import *\n",
    "from punisher.utils.dates import utc_to_epoch, epoch_to_utc\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "REDDIT_DIR = Path(cfg.DATA_DIR, 'reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price(df, field, asset, ex_id):\n",
    "    col_name = ohlcv_feed.get_col_name(field, asset.symbol, ex_id)\n",
    "    punisher.utils.charts.plot_range(\n",
    "        df, start=None, end=None, \n",
    "        column_name=col_name)\n",
    "\n",
    "def color_y_axis(ax, color):\n",
    "    \"\"\"Color your axes.\"\"\"\n",
    "    for t in ax.get_yticklabels():\n",
    "        t.set_color(color)\n",
    "    return None\n",
    "\n",
    "def plot_price_and_sentiment(df, sentiment_field, price_field):\n",
    "    fig, ax1 = plt.subplots(figsize=(24,18))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(df['utc'], df[price_field], color='r')\n",
    "    ax1.set_xlabel('time (utc)')\n",
    "    ax1.set_ylabel(price_field)\n",
    "    color_y_axis(ax1, 'r')\n",
    "    \n",
    "    ax2.plot(df['utc'], df[sentiment_field], color='blue')\n",
    "    ax2.set_ylabel(sentiment_field)\n",
    "    color_y_axis(ax2, 'b')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(price_field + \" vs \" + sentiment_field)\n",
    "    ax1.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def get_tweet_counts(df, freq='D'):\n",
    "    df = df.groupby(pd.Grouper(key='date', freq=freq)).size().to_frame()\n",
    "    df['epoch'] = [utc_to_epoch(d) for d in df.index]\n",
    "    df['utc'] = df.index\n",
    "    df.set_index('epoch', inplace=True)\n",
    "    df = df.rename(mapper={0:'tweets'}, axis='columns')\n",
    "    return df\n",
    "\n",
    "def get_tweet_sentiment(df, freq='D'):\n",
    "    df = df[['date', 'sentiment']]\n",
    "    df = df.groupby(pd.Grouper(key='date', freq='H')).mean()\n",
    "    df['epoch'] = [utc_to_epoch(d) for d in df.index]\n",
    "    df['utc'] = df.index\n",
    "    df.set_index('epoch', inplace=True)\n",
    "    df = df.rename(mapper={0:'sentiment'}, axis='columns')\n",
    "    return df\n",
    "\n",
    "def plot_tweets(df, freq='D'):\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "    tweets = get_tweet_counts(df, freq=freq)\n",
    "    fig = plt.figure(figsize=(24,18))\n",
    "    plt.plot(tweets['utc'], tweets['tweets'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Tweet Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coinmarketcap.com/api/\n",
    "exchange_ids = [ex_cfg.GDAX]#, ex_cfg.BINANCE]#, ex_cfg.POLONIEX]\n",
    "symbols = ['BTC/USD']#,'BTC/USDT']\n",
    "assets = [Asset.from_symbol(sym) for sym in symbols]\n",
    "start = datetime.datetime(year=2016, month=1, day=1)\n",
    "timeframe = Timeframe.ONE_DAY\n",
    "downloaded_df = ohlcv_feed.load_multiple_assets(\n",
    "    exchange_ids, assets, timeframe, start, end=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['open_BTC/USD_gdax', 'close_BTC/USD_gdax', 'utc']#,'close_BTC/USDT_binance']\n",
    "price_df = downloaded_df.copy()[columns]\n",
    "#df = df.rename(mapper={columns[i]:exchange_ids[i] for i in range(len(columns)-1)}, axis='columns')\n",
    "price_df.sort_values(by='utc').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(year=2016, month=1, day=1, hour=0)\n",
    "end = datetime.datetime(year=2018, month=2, day=15, hour=0)\n",
    "#times = [datetime(year=2018, month=2, day=1, hour=10, minute=i) for i in range(60)] + [datetime(year=2018, month=2, day=1, hour=11, minute=i) for i in range(60)]\n",
    "plot_df = price_df.copy()\n",
    "plot_df = plot_df[(plot_df['utc'] >= start) & (price_df['utc'] < end)]\n",
    "plot_df.plot(x='utc', figsize=(24,18), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = 'bitcoin'\n",
    "start = datetime.datetime(year=2016, month=1, day=1, hour=0)\n",
    "end = datetime.datetime(year=2018, month=2, day=15, hour=0)\n",
    "top_n_comments = 10\n",
    "subreddit = reddit_client.get_subreddit(subreddit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch from Reddit\n",
    "#subs = reddit_client.get_submissions(subreddit_name, start, end, top_n_comments)\n",
    "#subs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "# reddit_client.save_submissions(subs, subreddit_name, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "# subs = reddit_client.load_submissions(subreddit_name, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "n_top_subs = 25\n",
    "def load_top_n_subs(subreddit, start, n_top):\n",
    "    subs = reddit_client.load_submissions(subreddit, start)\n",
    "    return heapq.nlargest(n_top, subs, key=lambda s: s['score'])\n",
    "\n",
    "def get_sub_titles_df(subreddit, start, end, n_top_subs):\n",
    "    columns = ['epoch', 'utc'] + ['T{:d}'.format(i) for i in range(1, n_top_subs+1)]\n",
    "    top_subs = []\n",
    "    time_delta = datetime.timedelta(days=1)\n",
    "    cur_start = start\n",
    "    while cur_start < end:\n",
    "        subs = load_top_n_subs(subreddit, cur_start, n_top_subs)\n",
    "        titles = [sub['title'] for sub in subs]\n",
    "        top_subs.append([utc_to_epoch(cur_start), cur_start] + titles)\n",
    "        cur_start += time_delta\n",
    "    df = pd.DataFrame(top_subs, columns=columns)\n",
    "    df.set_index('epoch', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_subs = load_top_n_subs(subreddit_name, start, n_top_subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reddit headlines\n",
    "n_top_subs = 50\n",
    "subs_df = get_sub_titles_df(\n",
    "    subreddit_name, start, end, n_top_subs\n",
    ")\n",
    "subs_df = subs_df.fillna(value=\"\")\n",
    "#subs_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join price and reddit dataframes\n",
    "subs_columns = ['T{:d}'.format(i) for i in range(1, n_top_subs+1)]\n",
    "df = pd.concat([price_df, subs_df[subs_columns]], axis=1)\n",
    "df['label'] = df['close_BTC/USD_gdax'] > df['open_BTC/USD_gdax']\n",
    "df.sort_index(inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"[0-9]+\", \"number\", text)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"$[^\\s]+\", \"\", text)\n",
    "    text = re.sub(\"@[^\\s]+\", \"\", text)\n",
    "    text = re.sub(\"(http|https)://[^\\s]*\", \"\", text)\n",
    "    text = re.sub(\"[^\\s]+@[^\\s]+\", \"\", text)\n",
    "    text = re.sub('[^a-z A-Z]+', '', text)\n",
    "    return text\n",
    "\n",
    "def cleanup_titles(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleanup_titles(df, subs_columns)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency table for title\n",
    "example = df.iloc[0]['T1']\n",
    "example = CountVectorizer().build_tokenizer()(example)\n",
    "pd.DataFrame([[x, example.count(x)] for x in set(example)], columns = ['Word', 'Count']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "split_date = datetime.datetime(year=2017, month=11, day=1)\n",
    "train = df[df['utc'] < split_date]\n",
    "test = df[df['utc'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_columns = ['T{:d}'.format(i) for i in range(1, n_top_subs+1)]\n",
    "def concat_titles(df):\n",
    "    # Combine all titles into one string per day\n",
    "    titles = []\n",
    "    for row in range(0, len(df.index)):\n",
    "        titles.append(' '.join(str(x) for x in df.iloc[row][subs_columns]))\n",
    "    return titles\n",
    "\n",
    "def create_ngram_matrix(df, vectorizer, fit=True):\n",
    "    titles = concat_titles(df)\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n",
    "    # Returns spares matrix of word --> frequency\n",
    "    if fit:\n",
    "        titles = vectorizer.fit_transform(titles)\n",
    "    else:\n",
    "        titles = vectorizer.transform(titles)\n",
    "    return titles, vectorizer\n",
    "\n",
    "def get_coefficients(vectorizer, model):\n",
    "    # Inspect coefficients to see which words trigger predictions\n",
    "    words = vectorizer.get_feature_names()\n",
    "    coeffs = model.coef_.tolist()[0]\n",
    "    coeffdf = pd.DataFrame({'Word' : words, \n",
    "                            'Coefficient' : coeffs})\n",
    "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "    return coeffdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "* Place all titles into a \"bag\" and count the frequency of each word\n",
    "* http://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffdf = get_coefficients(vectorizer, model)\n",
    "coeffdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative coefficients mean a word is associates with lower price\n",
    "coeffdf.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "* Similar to bag-of-words (frequencies), but we count the frequency of word sequences\n",
    "* Bag of words = 1Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,3))\n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffdf = get_coefficients(vectorizer, model)\n",
    "coeffdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffdf.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancements?\n",
    "\n",
    "* Measure the distance and matching from some previously defined n-grams related to stock market\n",
    "* topic modeling LDA, Naive Bayes (https://www.kaggle.com/rahulvks/topic-modeling-using-gensim)\n",
    "* Remove common/uncommon words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.03, max_df=0.97, \n",
    "    max_features = 200000, ngram_range = (3, 3))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "coeffdf = get_coefficients(vectorizer, model)\n",
    "print(coeffdf.head(10))\n",
    "print(coeffdf.tail(10))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "coeffdf = get_coefficients(vectorizer, model).sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(coeffdf.head(10))\n",
    "print(coeffdf.tail(10))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier#, GradientBoostingClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.03, max_df=0.2, \n",
    "    max_features = 200000, ngram_range = (3, 3))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.02, max_df=0.175, \n",
    "    max_features = 200000, ngram_range = (2, 2))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.03, max_df=0.2, \n",
    "    max_features = 200000, ngram_range = (3, 3))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='modified_huber', max_iter=3, random_state=0, shuffle=True)\n",
    "model = model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.03, max_df=0.3, \n",
    "    max_features = 200000, ngram_range = (3, 3))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values\n",
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_input, train_target)\n",
    "preds = model.predict(test_input)\n",
    "print(\"Accuracy\", np.sum(preds == test_target)/len(preds))\n",
    "pd.crosstab(test_target, preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "* http://pytorch.org/docs/0.3.0/sparse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.04, max_df=0.3, \n",
    "    max_features = 200000, ngram_range = (2, 2))\n",
    "    \n",
    "train_input, vectorizer = create_ngram_matrix(train, vectorizer)\n",
    "train_target = train['label'].values\n",
    "test_input, _ = create_ngram_matrix(test, vectorizer, fit=False)\n",
    "test_target = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([670, 839]),\n",
       " torch.Size([106, 839]),\n",
       " torch.Size([670, 1]),\n",
       " torch.Size([106, 1]))"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = torch.from_numpy(train_input.toarray().astype('float32'))\n",
    "test_input = torch.from_numpy(test_input.toarray().astype('float32'))\n",
    "train_target = torch.from_numpy(np.expand_dims(train_target.astype('float32'),axis=1))\n",
    "test_target = torch.from_numpy(np.expand_dims(test_target.astype('float32'),axis=1))\n",
    "train_input.size(), test_input.size(), train_target.size(), test_target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "trn_dataset = TensorDataset(train_input, train_target)\n",
    "tst_dataset = TensorDataset(test_input, test_target)\n",
    "trn_loader = DataLoader(trn_dataset, batch_size, shuffle=True)\n",
    "tst_loader = DataLoader(tst_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as scipy_metrics\n",
    "\n",
    "def get_predictions(probs, thresholds):\n",
    "    preds = np.copy(probs)\n",
    "    preds[preds >= thresholds] = 1\n",
    "    preds[preds < thresholds] = 0\n",
    "    return preds.astype('uint8')\n",
    "\n",
    "def get_accuracy(preds, targets):\n",
    "    preds = preds.flatten() \n",
    "    targets = targets.flatten()\n",
    "    correct = np.sum(preds==targets)\n",
    "    return correct / len(targets)\n",
    "\n",
    "def get_recall(preds, targets):\n",
    "    return scipy_metrics.recall_score(targets.flatten(), preds.flatten())\n",
    "\n",
    "\n",
    "def get_precision(preds, targets):\n",
    "    return scipy_metrics.precision_score(targets.flatten(), preds.flatten())\n",
    "\n",
    "def run(model, optim, criterion, trn_loader, val_loader, n_epochs, n_classes):\n",
    "    for epoch in range(n_epochs):\n",
    "        trn_loss = train_model(model, trn_loader, optim, criterion, epoch, n_epochs)\n",
    "        val_loss, _, _ = predict(model, val_loader, criterion, n_classes)\n",
    "        print(\"Epoch {:d} Loss - Trn: {:.4f} | Val: {:4f}\".format(\n",
    "            epoch, trn_loss, val_loss))\n",
    "        \n",
    "def train_model(model, dataloader, optimizer, criterion, epoch, n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    loss_data = 0\n",
    "    n_batches = len(dataloader)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = Variable(inputs)#.cuda(async=True))\n",
    "        targets = Variable(targets)#.cuda(async=True))\n",
    "        \n",
    "        ## Forward Pass\n",
    "        output = model(inputs)\n",
    "\n",
    "        ## Clear Gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Metrics\n",
    "        loss = criterion(output, targets)\n",
    "        loss_data += loss.data[0]\n",
    "\n",
    "        ## Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_data / n_batches\n",
    "\n",
    "def predict(model, loader, criterion, n_classes):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    probs = np.empty((0, n_classes))\n",
    "    labels = np.empty((0, n_classes))\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs = Variable(inputs)#.cuda(async=True), volatile=True)\n",
    "        targets = Variable(targets)#.cuda(async=True), volatile=True)\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss += criterion(output, targets).data[0]\n",
    "        probs = np.vstack([probs, output.data.numpy()])\n",
    "        labels = np.vstack([labels, targets.data.numpy()])\n",
    "        \n",
    "    loss /= len(loader)\n",
    "    return loss, probs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return self.layers.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-499-37e9d6bd9d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "n_classes = 1\n",
    "n_features = train_input.shape[1]\n",
    "model = MLP(n_features, n_classes)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "out = model(Variable(train_input))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs,targets = next(iter(trn_loader))\n",
    "# inputs.size(),targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss - Trn: 0.6927 | Val: 0.691096\n",
      "Epoch 1 Loss - Trn: 0.6777 | Val: 0.695715\n",
      "Epoch 2 Loss - Trn: 0.6636 | Val: 0.696614\n",
      "Epoch 3 Loss - Trn: 0.6336 | Val: 0.685428\n",
      "Epoch 4 Loss - Trn: 0.5777 | Val: 0.671541\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "run(model, optim, criterion, trn_loader, tst_loader, n_epochs, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.556603773585\n",
      "Precision 0.558139534884\n",
      "Recall 0.842105263158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "Actual           \n",
       "0.0        11  38\n",
       "1.0         9  48"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, probs, labels = predict(model, tst_loader, criterion, n_classes, thresh=0.5)\n",
    "preds = get_predictions(probs, 0.5)\n",
    "assert test_target.shape == probs.shape\n",
    "probs[:5], preds[:5],labels[:5]\n",
    "print(\"Accuracy\", get_accuracy(preds, labels))\n",
    "print(\"Precision\", get_precision(preds, labels))\n",
    "print(\"Recall\", get_recall(preds, labels))\n",
    "pd.crosstab(labels.flatten(), preds.flatten(), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word Vectors/Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "* Word embeddings are a representation of the *semantics* of a word, efficiently encoding semantic information that might be relevant to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings (i.e. 5 neurons, randomly initialized)\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Turn the words into integer indices\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        log_probs = model(context_var)\n",
    "        loss = loss_function(log_probs, Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/pytorch/text\n",
    "* http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch\n",
    "* https://github.com/eladhoffer/seq2seq.pytorch/blob/master/seq2seq/tools/tokenizer.py\n",
    "* https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass through one at a time\n",
    "lstm = nn.LSTM(3,3) #input dim and output dim = 3\n",
    "inputs = [Variable(torch.randn(1,3)) for _ in range(5)]\n",
    "hidden = Variable(torch.randn(1,1,3)), Variable(torch.randn(1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass all inputs at once\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = Variable(torch.randn(1,1,3)), Variable(torch.randn(1,1,3))\n",
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input\n",
    "\n",
    "* https://www.kaggle.com/lseiyjg/use-news-to-predict-stock-markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction 1\n",
    "import torchtext\n",
    "from spacy.lang.en import English\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "batch_size = 32\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(670, 106)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_titles = concat_titles(train)\n",
    "train_labels = train['label']\n",
    "test_titles = concat_titles(test)\n",
    "test_labels = test['label']\n",
    "len(train_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = English().Defaults.create_tokenizer()\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_titles)\n",
    "sequences_train = tokenizer.texts_to_sequences(train_titles)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "train shape: (670, 200) (670, 2)\n",
      "test shape: (106, 200) (106, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "train_input = sequence.pad_sequences(sequences_train, maxlen=embedding_dim)\n",
    "test_input = sequence.pad_sequences(sequences_test, maxlen=embedding_dim)\n",
    "\n",
    "train_target = np_utils.to_categorical(train_labels, n_classes)\n",
    "test_target = np_utils.to_categorical(test_labels, n_classes)\n",
    "print('train shape:', train_input.shape, train_target.shape)\n",
    "print('test shape:', test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_inp = torch.FloatTensor(train_input)#.view(train_input.shape[0], 1, -1)\n",
    "tst_inp = torch.FloatTensor(test_input)#.view(test_input.shape[0], 1, -1)\n",
    "trn_targ = torch.FloatTensor(train_target)\n",
    "tst_targ = torch.FloatTensor(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "trn_dataset = TensorDataset(trn_inp, trn_targ)\n",
    "tst_dataset = TensorDataset(tst_inp, tst_targ)\n",
    "trn_loader = DataLoader(trn_dataset, batch_size, shuffle=True)\n",
    "tst_loader = DataLoader(tst_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, epoch, n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    loss_data = 0\n",
    "    n_batches = len(dataloader)\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = Variable(inputs)#.cuda(async=True))\n",
    "        targets = Variable(targets)#.cuda(async=True))\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden(len(inputs))\n",
    "        \n",
    "        ## Forward Pass\n",
    "        output = model(inputs)\n",
    "\n",
    "        ## Clear Gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Metrics\n",
    "        loss = criterion(output, targets)\n",
    "        loss_data += loss.data[0]\n",
    "\n",
    "        ## Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_data / n_batches\n",
    "\n",
    "def run(model, optim, criterion, trn_loader, val_loader, n_epochs, n_classes):\n",
    "    for epoch in range(n_epochs):\n",
    "        trn_loss = train_model(model, trn_loader, optim, criterion, epoch, n_epochs)\n",
    "        val_loss, _, _ = predict(model, val_loader, criterion, n_classes)\n",
    "        print(\"Epoch {:d} Loss - Trn: {:.4f} | Val: {:4f}\".format(\n",
    "            epoch, trn_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    # https://gist.github.com/spro/ef26915065225df65c1187562eca7ec4\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_classes):\n",
    "        super().__init__()\n",
    "        #self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=1,\n",
    "            dropout=0.5)\n",
    "        self.linear2 = nn.Linear(hidden_dim, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #out = self.embeddings(inputs)\n",
    "        #print(out.size())\n",
    "        out, self.hidden = self.lstm(inputs.view(1, len(inputs), -1), self.hidden)\n",
    "        out = self.linear2(out.view(len(inputs), -1))\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(200, 128, dropout=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(tokenizer.num_words, embedding_dim, hidden_dim, n_classes)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "criterion = F.binary_cross_entropy\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 200]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,targets = next(iter(trn_loader))\n",
    "inputs.size(),targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4788  0.5212\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Variable(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss - Trn: 0.6936 | Val: 0.714858\n",
      "Epoch 1 Loss - Trn: 0.6875 | Val: 0.723402\n",
      "Epoch 2 Loss - Trn: 0.6811 | Val: 0.708694\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "run(model, optim, criterion, trn_loader, tst_loader, n_epochs, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.490566037736\n",
      "Precision 0.516483516484\n",
      "Recall 0.824561403509\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "Actual           \n",
       "0           5  44\n",
       "1          10  47"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, probs, labels = predict(model, tst_loader, criterion, n_classes)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "labels = np.argmax(labels, axis=1)\n",
    "print(\"Accuracy\", get_accuracy(preds, labels))\n",
    "print(\"Precision\", get_precision(preds, labels))\n",
    "print(\"Recall\", get_recall(preds, labels))\n",
    "pd.crosstab(labels.flatten(), preds.flatten(), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Price Predictions\n",
    "\n",
    "* https://www.kaggle.com/katerynad/bernoulli-naive-bayes-auc-59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Indicators\n",
    "\n",
    "* https://www.kaggle.com/hiteshp/money-money-share-market-study/notebook\n",
    "* https://www.kaggle.com/hiteshp/money-money-share-market-study-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM\n",
    "\n",
    "* http://amunategui.github.io/markov-chains/index.html\n",
    "* https://www.kaggle.com/hiteshp/hidden-markov-model-predicting-stock-market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "* https://github.com/abdulfatir/twitter-sentiment-analysis\n",
    "* http://textblob.readthedocs.io/en/dev/\n",
    "* https://github.com/fnielsen/afinn\n",
    "* https://github.com/anfederico/Stocktalk/blob/master/stocktalk/scripts/streaming.py\n",
    "* http://www.nltk.org/\n",
    "* http://textblob.readthedocs.io/en/dev/quickstart.html#translation-and-language-detection\n",
    "* https://github.com/juvaroka/tweetwise\n",
    "* https://www.kaggle.com/shreyams/stock-price-prediction-94-xgboost (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.polarity\n",
    "    \n",
    "def add_sentiment_score(df, columns):\n",
    "    for col in columns:\n",
    "        texts = df[col].tolist()\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            score = get_sentiment(text)\n",
    "            scores.append(score)\n",
    "        df['sentiment_'+col] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentiment\n",
    "add_sentiment_score(df, columns=subs_columns)\n",
    "sentiment_cols = ['sentiment_'+col for col in subs_columns]\n",
    "df['sentiment'] = df[sentiment_cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Change\n",
    "open_col = ohlcv_feed.get_col_name('open', assets[0].symbol, exchange_ids[0])\n",
    "close_col = ohlcv_feed.get_col_name('close', assets[0].symbol, exchange_ids[0])\n",
    "df['price_delta'] = (df[close_col] - df[open_col])/df[open_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Price Change vs Sentiment\n",
    "plot_price_and_sentiment(df, 'sentiment', 'price_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "# http://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/\n",
    "# Most statisticians like to see correlations beyond at least +0.5 or –0.5 before getting too excited about them\n",
    "print(np.corrcoef(df['price_delta'], df['sentiment']))\n",
    "print(np.correlate(df['price_delta'], df['sentiment'])[0])\n",
    "plt.matshow(df[['price_delta','sentiment']].corr())\n",
    "pd.scatter_matrix(df[['price_delta','sentiment']], \n",
    "                  alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "71px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
